"""
Module: generate_test_summary.py
Description: Test suite for generate_summary functionality

Sample Input:
>>> # See function docstrings for specific examples

Expected Output:
>>> # See function docstrings for expected results

Example Usage:
>>> # Import and use as needed based on module functionality
"""

import sys
from pathlib import Path

# Add src to path for imports
src_path = Path(__file__).parent.parent / "src"
if src_path.exists() and str(src_path) not in sys.path:
    sys.path.insert(0, str(src_path))



#!/usr/bin/env python3
"""
Generate Markdown test report from pytest JSON output.

This script parses the JSON report generated by pytest-json-report
and creates a well-formatted Markdown report with test results.
"""

import json
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any


def parse_test_results(json_file: str) -> Dict[str, Any]:
    """Parse pytest JSON report file."""
    with open(json_file, 'r') as f:
        return json.load(f)


def format_duration(seconds: float) -> str:
    """Format duration in seconds to human-readable format."""
    if seconds < 1:
        return f"{seconds*1000:.1f}ms"
    elif seconds < 60:
        return f"{seconds:.2f}s"
    else:
        minutes = int(seconds // 60)
        secs = seconds % 60
        return f"{minutes}m {secs:.1f}s"


def get_test_status_emoji(outcome: str) -> str:
    """Return emoji for test status."""
    status_map = {
        "passed": "",
        "failed": "",
        "skipped": "⏭️",
        "error": "⚠️",
        "xfailed": "",
        "xpassed": ""
    }
    return status_map.get(outcome, "")


def generate_markdown_report(data: Dict[str, Any], output_file: str) -> None:
    """Generate Markdown report from test data."""
    report_lines = []
    
    # Header
    report_lines.append("# ArangoDB Test Report")
    report_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_lines.append("")
    
    # Summary section
    summary = data.get('summary', {})
    total = summary.get('total', 0)
    passed = summary.get('passed', 0)
    failed = summary.get('failed', 0)
    skipped = summary.get('skipped', 0)
    error = summary.get('error', 0)
    duration = data.get('duration', 0)
    
    success_rate = (passed / total * 100) if total > 0 else 0
    
    report_lines.append("## Test Summary")
    report_lines.append("")
    report_lines.append(f"**Total Tests**: {total}")
    report_lines.append(f"**Passed**: {passed} ")
    report_lines.append(f"**Failed**: {failed} ")
    report_lines.append(f"**Skipped**: {skipped} ⏭️")
    report_lines.append(f"**Errors**: {error} ⚠️")
    report_lines.append(f"**Success Rate**: {success_rate:.1f}%")
    report_lines.append(f"**Total Duration**: {format_duration(duration)}")
    report_lines.append("")
    
    # Test results table
    report_lines.append("## Test Results")
    report_lines.append("")
    report_lines.append("| Test Name | Module | Status | Duration | Error |")
    report_lines.append("|-----------|--------|--------|----------|-------|")
    
    # Process test results
    tests = data.get('tests', [])
    for test in tests:
        nodeid = test.get('nodeid', '')
        # Extract test name and module from nodeid
        parts = nodeid.split('::')
        if len(parts) >= 2:
            module = parts[0].replace('tests/', '')
            test_name = '::'.join(parts[1:])
        else:
            module = 'unknown'
            test_name = nodeid
        
        outcome = test.get('outcome', 'unknown')
        status = f"{get_test_status_emoji(outcome)} {outcome}"
        duration = format_duration(test.get('duration', 0))
        
        # Get error message if test failed
        error_msg = ""
        if outcome in ['failed', 'error']:
            call = test.get('call', {})
            if 'longrepr' in call:
                error_msg = str(call['longrepr']).split('\n')[0][:50] + "..."
        
        report_lines.append(f"| {test_name} | {module} | {status} | {duration} | {error_msg} |")
    
    # Failed test details
    failed_tests = [t for t in tests if t.get('outcome') in ['failed', 'error']]
    if failed_tests:
        report_lines.append("")
        report_lines.append("## Failed Test Details")
        report_lines.append("")
        
        for test in failed_tests:
            nodeid = test.get('nodeid', '')
            report_lines.append(f"### {nodeid}")
            report_lines.append("")
            
            call = test.get('call', {})
            if 'longrepr' in call:
                report_lines.append("```")
                report_lines.append(str(call['longrepr']))
                report_lines.append("```")
                report_lines.append("")
    
    # Test coverage section (if available)
    if 'coverage' in data:
        coverage = data['coverage']
        report_lines.append("## Code Coverage")
        report_lines.append("")
        report_lines.append(f"**Total Coverage**: {coverage.get('percent', 0):.1f}%")
        report_lines.append("")
    
    # Environment information
    env = data.get('environment', {})
    if env:
        report_lines.append("## Environment")
        report_lines.append("")
        report_lines.append(f"- Python: {env.get('Python', 'unknown')}")
        report_lines.append(f"- Platform: {env.get('Platform', 'unknown')}")
        report_lines.append("")
    
    # Write report to file
    with open(output_file, 'w') as f:
        f.write('\n'.join(report_lines))
    
    print(f"Report generated: {output_file}")
    
    # Also print summary to console
    print("\nTest Summary:")
    print(f"  Total: {total}")
    print(f"  Passed: {passed} ({success_rate:.1f}%)")
    print(f"  Failed: {failed}")
    print(f"  Skipped: {skipped}")
    print(f"  Duration: {format_duration(duration)}")


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Generate test report from pytest JSON output')
    parser.add_argument('--json-file', required=True, help='Path to pytest JSON report file')
    parser.add_argument('--output-file', required=True, help='Path to output Markdown report')
    
    args = parser.parse_args()
    
    # Ensure output directory exists
    output_path = Path(args.output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Parse and generate report
    try:
        data = parse_test_results(args.json_file)
        generate_markdown_report(data, args.output_file)
    except Exception as e:
        print(f"Error generating report: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())